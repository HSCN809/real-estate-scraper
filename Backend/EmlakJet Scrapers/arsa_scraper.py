import time
import json
import csv
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.chrome.options import Options

class ArsaScraper:
    def __init__(self, driver, base_url, selected_locations=None):
        self.driver = driver
        self.base_url = base_url
        self.selected_locations = selected_locations or {}
        self.all_listings = []
        self.wait = WebDriverWait(self.driver, 10)
        
    def start_scraping(self):
        """Scraping iÅŸlemini baÅŸlat"""
        print(f"ğŸš€ Arsa Scraper baÅŸlatÄ±lÄ±yor: {self.base_url}")
        
        try:
            # KullanÄ±cÄ±dan sayfa sayÄ±sÄ±nÄ± al
            max_pages = self.get_user_page_count()
            if max_pages is None:
                return
                
            # SayfalarÄ± tara
            self.scrape_pages(max_pages)
            
            # Verileri kaydet
            self.save_data()
            
            print(f"\nâœ… Scraping tamamlandÄ±! Toplam {len(self.all_listings)} ilan bulundu.")
            
        except Exception as e:
            print(f"âŒ Scraping sÄ±rasÄ±nda hata: {e}")
    
    def get_user_page_count(self):
        """KullanÄ±cÄ±dan kaÃ§ sayfa taranacaÄŸÄ±nÄ± al"""
        try:
            print(f"\nğŸ“„ Maksimum sayfa sayÄ±sÄ±nÄ± Ã¶ÄŸreniliyor...")
            max_available_pages = self.get_max_pages()
            print(f"ğŸ“Š Sitede toplam {max_available_pages} sayfa bulunuyor.")
            
            while True:
                try:
                    user_input = input(f"\nğŸ”¢ KaÃ§ sayfa taranacak? (1-{max_available_pages}): ").strip()
                    
                    if not user_input:
                        print("âŒ GeÃ§ersiz giriÅŸ! LÃ¼tfen bir sayÄ± girin.")
                        continue
                    
                    page_count = int(user_input)
                    
                    if page_count < 1:
                        print("âŒ En az 1 sayfa seÃ§melisiniz!")
                        continue
                    
                    if page_count > max_available_pages:
                        print(f"âŒ Maksimum {max_available_pages} sayfa seÃ§ebilirsiniz!")
                        continue
                    
                    print(f"âœ… {page_count} sayfa taranacak...")
                    return page_count
                    
                except ValueError:
                    print("âŒ GeÃ§ersiz giriÅŸ! LÃ¼tfen bir sayÄ± girin.")
                except KeyboardInterrupt:
                    print("\nâ¹ï¸  Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan iptal edildi.")
                    return None
                    
        except Exception as e:
            print(f"âŒ Sayfa sayÄ±sÄ± alÄ±nÄ±rken hata: {e}")
            return 1
    
    def scrape_pages(self, max_pages):
        """Belirtilen sayÄ±da sayfayÄ± tarar"""
        for current_page in range(1, max_pages + 1):
            print(f"\nğŸ” Sayfa {current_page} taranÄ±yor...")
            
            try:
                # Sayfaya git
                page_url = f"{self.base_url}?sayfa={current_page}" if current_page > 1 else self.base_url
                self.driver.get(page_url)
                time.sleep(2)
                
                # Ä°lanlarÄ± Ã§ek
                listings = self.scrape_current_page()
                self.all_listings.extend(listings)
                
                print(f"   âœ… Sayfa {current_page}: {len(listings)} ilan bulundu")
                
            except Exception as e:
                print(f"   âŒ Sayfa {current_page} taranÄ±rken hata: {e}")
                continue
    
    def get_max_pages(self):
        """Maksimum sayfa sayÄ±sÄ±nÄ± bul"""
        try:
            self.driver.get(self.base_url)
            time.sleep(2)
            
            # Sayfalama elementlerini bul
            pagination = self.driver.find_elements(By.CSS_SELECTOR, "ul.styles_list__zqOeW li")
            
            if not pagination:
                return 1
            
            # Sayfa numaralarÄ±nÄ± topla
            page_numbers = []
            for item in pagination:
                try:
                    # Aktif sayfa
                    active_page = item.find_element(By.CSS_SELECTOR, "span.styles_selected__hilA_")
                    page_numbers.append(int(active_page.text))
                except:
                    pass
                
                try:
                    # Link sayfalarÄ±
                    page_link = item.find_element(By.CSS_SELECTOR, "a")
                    page_text = page_link.text
                    if page_text.isdigit():
                        page_numbers.append(int(page_text))
                except:
                    pass
            
            return max(page_numbers) if page_numbers else 1
            
        except Exception as e:
            print(f"âŒ Sayfa sayÄ±sÄ± alÄ±nÄ±rken hata: {e}")
            return 1
    
    def scrape_current_page(self):
        """Mevcut sayfadaki ilanlarÄ± Ã§eker"""
        listings = []
        
        try:
            # Ä°lan container'larÄ±nÄ± bul
            listing_containers = self.driver.find_elements(By.CSS_SELECTOR, "a.styles_wrapper__587DT")
            
            for container in listing_containers:
                try:
                    listing_data = self.extract_listing_data(container)
                    if listing_data:
                        listings.append(listing_data)
                        
                except Exception:
                    continue
            
        except Exception:
            pass
        
        return listings
    
    def extract_listing_data(self, container):
        """Tek bir ilanÄ±n verilerini Ã§Ä±karÄ±r - ARSA Ã–ZEL"""
        try:
            # HTML'DE GÃ–RDÃœÄÃœMÃœZ TEMEL BÄ°LGÄ°LER
            title = self.get_element_text(container, "h3.styles_title__aKEGQ")
            location = self.get_element_text(container, "span.styles_location__OwJiQ")
            price = self.get_element_text(container, "span.styles_price__F3pMQ")
            
            # Arsa iÃ§in Ã¶zel quick info (Tarla | 2.821 mÂ²)
            quick_info = self.get_element_text(container, "div.styles_quickinfoWrapper__Vsnk5")
            
            # GÃ¶rsel URL
            image_url = self.get_element_attribute(container, "img.styles_imageClass___SLvt", "src")
            
            # Ä°lan URL
            listing_url = container.get_attribute("href")
            
            # Badge bilgileri
            badges = self.extract_badges(container)
            
            # Arsa Ã¶zel detaylarÄ± parse et
            details = self.parse_arsa_details(quick_info, title)
            
            listing_data = {
                'baslik': title,
                'lokasyon': location,
                'fiyat': price,
                'ilan_url': listing_url,
                'resim_url': image_url,
                'one_cikan': 'Ã–NE Ã‡IKAN' in badges,
                'yeni': 'YENÄ°' in badges,
                'arsa_tipi': details['arsa_tipi'],
                'metrekare': details['metrekare'],
                'imar_durumu': details['imar_durumu'],
                'tarih': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # Temel bilgiler eksikse atla
            if not all([title, location, price]):
                return None
                
            return listing_data
            
        except Exception:
            return None
    
    def parse_arsa_details(self, quick_info, title):
        """Arsa Ã¶zel detaylarÄ±nÄ± parse et"""
        details = {
            'arsa_tipi': '',
            'metrekare': '',
            'imar_durumu': ''
        }
        
        # Quick info'dan arsa tipi ve metrekare
        if quick_info:
            try:
                # "Tarla | 2.821 mÂ²" formatÄ±nÄ± parse et
                parts = [part.strip() for part in quick_info.split('|')]
                
                for part in parts:
                    if any(tip in part.lower() for tip in ['tarla', 'arsa', 'arazi', 'bahÃ§e', 'zeytinlik']):
                        details['arsa_tipi'] = part
                    elif 'mÂ²' in part or 'm2' in part.lower():
                        details['metrekare'] = part
            except:
                pass
        
        # BaÅŸlÄ±ktan imar durumu Ã§Ä±kar
        if title:
            title_lower = title.lower()
            if 'imar' in title_lower:
                if 'imarÄ± yok' in title_lower or 'imarsÄ±z' in title_lower:
                    details['imar_durumu'] = 'Ä°marsÄ±z'
                elif 'imarÄ± var' in title_lower or 'imarlÄ±' in title_lower:
                    details['imar_durumu'] = 'Ä°marlÄ±'
            elif 'tapulu' in title_lower:
                details['imar_durumu'] = 'Tapulu'
            elif 'kat' in title_lower and 'karÅŸÄ±lÄ±ÄŸÄ±' in title_lower:
                details['arsa_tipi'] = 'Kat KarÅŸÄ±lÄ±ÄŸÄ± Arsa'
        
        return details
    
    def extract_badges(self, container):
        """Badge bilgilerini Ã§Ä±karÄ±r"""
        badges = []
        try:
            badge_elements = container.find_elements(By.CSS_SELECTOR, "div.styles_badgewrapper__pS0rt")
            for badge in badge_elements:
                badge_text = badge.text.strip()
                if badge_text:
                    badges.append(badge_text)
        except:
            pass
        return badges
    
    def get_element_text(self, container, selector):
        """Element metnini al"""
        try:
            element = container.find_element(By.CSS_SELECTOR, selector)
            return element.text.strip()
        except:
            return ""
    
    def get_element_attribute(self, container, selector, attribute):
        """Element attribute deÄŸerini al"""
        try:
            element = container.find_element(By.CSS_SELECTOR, selector)
            return element.get_attribute(attribute)
        except:
            return ""
    
    def save_data(self):
        """Verileri JSON ve CSV formatÄ±nda kaydet"""
        if not self.all_listings:
            print("âŒ Kaydedilecek veri bulunamadÄ±!")
            return
        
        # KlasÃ¶r oluÅŸtur
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        folder_name = f"scraped_arsa_data_{timestamp}"
        os.makedirs(folder_name, exist_ok=True)
        
        # JSON kaydet
        json_filename = os.path.join(folder_name, "arsa_ilanlari.json")
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(self.all_listings, f, ensure_ascii=False, indent=2)
        
        # CSV kaydet
        csv_filename = os.path.join(folder_name, "arsa_ilanlari.csv")
        self.save_to_csv(csv_filename)
        
        print(f"ğŸ’¾ Veriler kaydedildi:")
        print(f"   ğŸ“„ JSON: {json_filename}")
        print(f"   ğŸ“Š CSV: {csv_filename}")
    
    def save_to_csv(self, filename):
        """Verileri CSV formatÄ±nda kaydet - ARSA Ã–ZEL"""
        if not self.all_listings:
            return
        
        # Arsa Ã¶zel sÃ¼tunlar
        fieldnames = [
            'baslik', 'lokasyon', 'fiyat', 'ilan_url', 'resim_url',
            'one_cikan', 'yeni', 'arsa_tipi', 'metrekare', 'imar_durumu', 'tarih'
        ]
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for listing in self.all_listings:
                writer.writerow(listing)

def setup_driver():
    """Chrome driver'Ä± sessiz modda baÅŸlat"""
    chrome_options = Options()
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    chrome_options.add_experimental_option("useAutomationExtension", False)
    
    # TÃ¼m loglarÄ± kapat
    chrome_options.add_argument("--log-level=3")
    chrome_options.add_argument("--disable-logging")
    chrome_options.add_argument("--disable-dev-tools")
    
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    chrome_options.add_argument(f"user-agent={user_agent}")
    
    driver = webdriver.Chrome(options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver

# Test iÃ§in standalone Ã§alÄ±ÅŸtÄ±rma
def test_scraper():
    """Test fonksiyonu"""
    driver = setup_driver()
    
    try:
        # Test URL'si
        test_url = "https://www.emlakjet.com/satilik-arsa"
        
        scraper = ArsaScraper(driver, test_url)
        scraper.start_scraping()
        
    except Exception as e:
        print(f"âŒ Test sÄ±rasÄ±nda hata: {e}")
    finally:
        driver.quit()

if __name__ == "__main__":
    test_scraper()